{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   The above code iterates through a subset of responses containing questions and answers, tokenizes them, and computes the negative log likelihoods using a pre-trained model. \n",
    "-   It then calculates the Area Under the Receiver Operating Characteristic (AUROC) for the probabilities of true answers and saves this value to a file. \n",
    "-   The code is part of an evaluation process to assess the model's ability to determine the correctness of generated answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1135b6454d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Please make sure you are using CUDA enabled GPU for this project\n",
    "device = 'cuda'\n",
    "\n",
    "# Setting the seed value ensures that the results are reproducible across different runs\n",
    "seed_val = 10\n",
    "\n",
    "# Ensuring that the seed is set for Python's hashing, random operations, NumPy, and PyTorch\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_val)\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Parameter Tuning\n",
    "params = {    \n",
    "    # Model related arguments\n",
    "    'generation_version': 'opt-125m',\n",
    "    'experiment_id_for_few_shot': 'run_1',\n",
    "    'experiment_id': 'run_1',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m4data692\u001b[0m (\u001b[33mnlp53113\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\dhair\\Desktop\\Data\\UChicago\\Morningstar\\NLPLearning\\Project\\wandb\\run-20230813_191418-run_1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/nlp53113/nlg_uncertainty/runs/run_1' target=\"_blank\">run_1</a></strong> to <a href='https://wandb.ai/nlp53113/nlg_uncertainty' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp53113/nlg_uncertainty' target=\"_blank\">https://wandb.ai/nlp53113/nlg_uncertainty</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp53113/nlg_uncertainty/runs/run_1' target=\"_blank\">https://wandb.ai/nlp53113/nlg_uncertainty/runs/run_1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# We are using wandb to track our experiments\n",
    "wandb.init(project='nlg_uncertainty', id=params['experiment_id_for_few_shot'], config=params, resume='allow')\n",
    "model_name = wandb.config.generation_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect garbage to free up CUDA memory\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5384437760, 6441926656)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please use the models below you have some free memory in your GPU\n",
    "# If you don't have enough memory, then it can lead to a crash (will require restart of the kernel/system)\n",
    "torch.cuda.mem_get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Getting the model from params and loading it to the GPU\n",
    "# Since we will be using the same model for other notebooks, we will save it in the cache directory\n",
    "generation_tokenizer = AutoTokenizer.from_pretrained(f\"facebook/opt-350m\", use_fast=False, cache_dir='./cache_dir')\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"facebook/{model_name}\", torch_dtype=torch.float16, cache_dir='./cache_dir').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_version = wandb.run.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Please run the cleaner notebook before running this notebook\n",
    "# Load the responses for the given run and model version\n",
    "with open(f'./sequences/{run_version}/{model_name}_cleaned_generations.pkl', 'rb') as infile:\n",
    "    responses_for_few_shot_prompt = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a subset of sequences for few-shot prompts\n",
    "subset_of_sequences_for_few_shot_prompt = sequences_for_few_shot_prompt[-10:]\n",
    "number_of_few_shot_samples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating template for the few-shot prompt\n",
    "prompt_template = 'Question: {} \\n Here are some ideas that were brainstormed:{}\\n Possible answer:{}\\n Is the possible answer:\\n (A) True\\n (B) False\\n The possible answer is:'\n",
    "few_shot_promopt = ''\n",
    "\n",
    "\n",
    "# Iterating through the subset of sequences to create the few-shot prompt\n",
    "for sequence in subset_of_sequences_for_few_shot_prompt:\n",
    "    question_text = sequence['question']\n",
    "    question_text = question_text.split('Question: ')[-1].split('Answer: ')[0]\n",
    "    prompt_text = sequence['prompt']\n",
    "    generated_ideas = '\\n'.join(sequence['cleaned_generated_texts'][:number_of_few_shot_samples])\n",
    "    most_probable_answer = sequence['most_likely_generation']\n",
    "    is_correct = ' True' if sequence['rougeL_to_target'] > 0.3 else ' False'\n",
    "    \n",
    "    # Appending the formatted prompt to the few_shot_prompt\n",
    "    few_shot_prompt += prompt_template.format(question_text, generated_ideas, most_probable_answer) + is_correct + '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing lists to store labels and probabilities across datasets\n",
    "labels_across_datasets = []\n",
    "p_trues_across_datasets = []\n",
    "\n",
    "# Defining the number of samples to be used\n",
    "n_samples_to_use = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:00<00:00, 16.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure that no gradients are computed during this block, for efficiency\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Lists to store Area Under the Receiver Operating Characteristic (AUROC) values, probabilities of true, and correctness flags\n",
    "    auroc_values = []\n",
    "    probabilities_true = []\n",
    "    correctness_flags = []\n",
    "    \n",
    "    # Loop through the responses to compute the negative log likelihoods\n",
    "    for response in tqdm(responses_for_few_shot_prompt[:n_samples_to_use]):\n",
    "\n",
    "        # Extract and clean the question text from the response\n",
    "        question_text = response['question']\n",
    "        if 'Question: ' in question_text:\n",
    "            question_text = question_text.split('Question: ')[-1].split('Answer: ')[0]\n",
    "        else:\n",
    "            question_text = question_text.split('Q: ')[-1].split('A: ')[0]\n",
    "\n",
    "        # Extract generated texts and the most likely answer\n",
    "        generated_ideas = '\\n'.join(response['cleaned_generated_texts'][:number_of_few_shot_samples])\n",
    "        most_probable_answer = response['most_likely_generation']\n",
    "        \n",
    "        # Determine if the answer is correct based on the rougeL_to_target metric\n",
    "        is_correct = 1.0 if response['rougeL_to_target'] > 0.3 else 0.0\n",
    "\n",
    "        # Construct the base and true prompts\n",
    "        base_prompt_text = prompt_template.format(question_text, generated_ideas, most_probable_answer)\n",
    "        prompt_with_true_answer = few_shot_prompt + prompt_template.format(question_text, generated_ideas, most_probable_answer) + ' True'\n",
    "\n",
    "        # This computation of the negative log likelihoods follows this tutorial: https://huggingface.co/docs/transformers/perplexity\n",
    "        # Tokenize the prompts for the model\n",
    "        tokenized_base_prompt = generation_tokenizer(base_prompt_text)['input_ids']\n",
    "        tokenized_prompt_with_true_answer = torch.tensor(generation_tokenizer(prompt_with_true_answer)['input_ids'], device=device)\n",
    "\n",
    "        # Prepare target IDs for the model\n",
    "        target_ids_with_true_answer = tokenized_prompt_with_true_answer.clone()\n",
    "        target_ids_with_true_answer[:len(tokenized_base_prompt)] = -100\n",
    "\n",
    "        # Compute the model's output and loss for the true prompt\n",
    "        model_output_with_true_answer = model(torch.reshape(tokenized_prompt_with_true_answer, (1, -1)), labels=target_ids_with_true_answer)\n",
    "        loss_with_true_answer = model_output_with_true_answer.loss\n",
    "\n",
    "        # Append the computed values to the lists\n",
    "        probabilities_true.append(loss_with_true_answer.item())\n",
    "        correctness_flags.append(is_correct)\n",
    "\n",
    "        labels_across_datasets += correctness_flags\n",
    "        probabilities_of_true_across_datasets += probabilities_true\n",
    "\n",
    "    # Compute the AUROC for the probabilities of true\n",
    "    auroc_for_true = metrics.roc_auc_score(1 - torch.tensor(correctness_flags), torch.tensor(probabilities_true))\n",
    "\n",
    "\n",
    "    # Store p_true aurocs in a pickle file\n",
    "    with open(f'./uncertainity/{run_version}/{model_name}_p_true_aurocs.pkl', 'wb') as outfile:\n",
    "        pickle.dump(p_true_auroc, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "pathlib.Path(f'./uncertainity/' + run_version).mkdir(parents=True, exist_ok=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
