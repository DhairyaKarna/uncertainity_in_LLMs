{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Parameter Tuning\n",
    "params = {    \n",
    "    # Model related arguments\n",
    "    'generation_version': 'opt-125m',\n",
    "    'experiment_id': 'run_1',\n",
    "    'verbose': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store overall results for each model\n",
    "overall_result_dict = {}\n",
    "\n",
    "# List to store Area Under the Receiver Operating Characteristic (AUROC) for each model\n",
    "aurocs_across_models = []\n",
    "\n",
    "# Dictionary to store embeddings for each sequence\n",
    "sequence_embeddings_dict = {}\n",
    "\n",
    "# Extracting run IDs that need to be analyzed from the arguments\n",
    "run_version = params['run_version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run_1'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:run_1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45550b4a1a9f4e6d82fb99a67d77a650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.3988</td></tr><tr><td>average_neg_llh_most_likely_gen_auroc</td><td>0.59308</td></tr><tr><td>average_rougeL_among_generations</td><td>0.11742</td></tr><tr><td>average_rougeL_among_generations_correct</td><td>0.13015</td></tr><tr><td>average_rougeL_among_generations_incorrect</td><td>0.10897</td></tr><tr><td>average_rougeL_auroc</td><td>0.49073</td></tr><tr><td>entropy_over_concepts_auroc</td><td>0.62563</td></tr><tr><td>ln_predictive_entropy_auroc</td><td>0.62683</td></tr><tr><td>margin_measure_auroc</td><td>0.58221</td></tr><tr><td>model_name</td><td>opt-350m</td></tr><tr><td>neg_llh_most_likely_gen_auroc</td><td>0.58449</td></tr><tr><td>number_of_semantic_sets_auroc</td><td>0.58246</td></tr><tr><td>number_of_semantic_sets_correct</td><td>1.33613</td></tr><tr><td>number_of_semantic_sets_incorrect</td><td>1.57583</td></tr><tr><td>predictive_entropy_auroc</td><td>0.60187</td></tr><tr><td>rougeL_based_accuracy</td><td>0.3988</td></tr><tr><td>run_name</td><td>run_1</td></tr><tr><td>unnormalised_entropy_over_concepts_auroc</td><td>0.60835</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run_1</strong> at: <a href='https://wandb.ai/nlp53113/nlg_uncertainty/runs/run_1' target=\"_blank\">https://wandb.ai/nlp53113/nlg_uncertainty/runs/run_1</a><br/>Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230813_020053-run_1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:run_1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c91a8d850624ca9af18a0938237383d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\dhair\\Desktop\\Data\\UChicago\\Morningstar\\NLPLearning\\Project\\wandb\\run-20230813_020444-run_1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/nlp53113/nlg_uncertainty/runs/run_1' target=\"_blank\">run_1</a></strong> to <a href='https://wandb.ai/nlp53113/nlg_uncertainty' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp53113/nlg_uncertainty' target=\"_blank\">https://wandb.ai/nlp53113/nlg_uncertainty</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp53113/nlg_uncertainty/runs/run_1' target=\"_blank\">https://wandb.ai/nlp53113/nlg_uncertainty/runs/run_1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_1\n",
      "opt-125m\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# We are using wandb to track our experiments\n",
    "wandb.init(project='nlg_uncertainty', id=run_ids_to_analyze, resume='allow')\n",
    "\n",
    "model_name = wandb.config.model\n",
    "print(run_name)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_similarity_dataframe():\n",
    "    \"\"\"\n",
    "    Load the similarity data from a pickle file and transform it into a DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A pandas DataFrame containing similarity data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct the file path based on run and model names\n",
    "    file_path = f'./ss/{run_version}/{model_name}_generations_similarities.pkl'\n",
    "    \n",
    "    # Load the pickle file into a dictionary\n",
    "    with open(file_path, 'rb') as file:\n",
    "        similarity_data = pickle.load(file)\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    similarity_dataframe = pd.DataFrame.from_dict(similarity_data, orient='index')\n",
    "    \n",
    "    # Add an 'id' column based on the DataFrame index\n",
    "    similarity_dataframe['id'] = similarity_dataframe.index\n",
    "    \n",
    "    # Convert the 'has_semantically_different_answers' column to integer type\n",
    "    similarity_dataframe['has_semantically_different_answers'] = similarity_dataframe['has_semantically_different_answers'].astype('int')\n",
    "    \n",
    "    # Extract the 'rougeL' value from the 'syntactic_similarities' column\n",
    "    similarity_dataframe['rougeL_among_generations'] = similarity_dataframe['syntactic_similarities'].apply(lambda x: x['rougeL'])\n",
    "\n",
    "    return similarity_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_generations_dataframe():\n",
    "    \"\"\"\n",
    "    Load the generations data from a pickle file and transform it into a DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A pandas DataFrame containing generation data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct the file path based on run and model names\n",
    "    file_path = f'./sequences/{run_name}/{model_name}_generations.pkl'\n",
    "    \n",
    "    # Load the pickle file into a list\n",
    "    with open(file_path, 'rb') as file:\n",
    "        generation_data = pickle.load(file)\n",
    "    \n",
    "    # Convert the list to a DataFrame\n",
    "    generation_dataframe = pd.DataFrame(generation_data)\n",
    "    \n",
    "    # Process 'id' column\n",
    "    generation_dataframe['id'] = generation_dataframe['id'].apply(lambda x: x[0])\n",
    "    generation_dataframe['id'] = generation_dataframe['id'].astype('object')\n",
    "    \n",
    "    # Process 'semantic_variability_reference_answers' column if no null values\n",
    "    if not generation_dataframe['semantic_variability_reference_answers'].isnull().values.any():\n",
    "        generation_dataframe['semantic_variability_reference_answers'] = generation_dataframe['semantic_variability_reference_answers'].apply(lambda x: x[0].item())\n",
    "    \n",
    "    # Process 'rougeL_reference_answers' column if no null values\n",
    "    if not generation_dataframe['rougeL_reference_answers'].isnull().values.any():\n",
    "        generation_dataframe['rougeL_reference_answers'] = generation_dataframe['rougeL_reference_answers'].apply(lambda x: x[0].item())\n",
    "    \n",
    "    # Calculate the length of the most likely generation and the answer\n",
    "    generation_dataframe['length_of_most_likely_generation'] = generation_dataframe['most_likely_generation'].apply(lambda x: len(str(x).split(' ')))\n",
    "    generation_dataframe['length_of_answer'] = generation_dataframe['answer'].apply(lambda x: len(str(x).split(' ')))\n",
    "    \n",
    "    # Calculate the variance of the length of generations\n",
    "    generation_dataframe['variance_of_length_of_generations'] = generation_dataframe['generated_texts'].apply(lambda x: np.var([len(str(y).split(' ')) for y in x]))\n",
    "    \n",
    "    # Determine if the generation is correct based on 'rougeL_to_target'\n",
    "    generation_dataframe['correct'] = (generation_dataframe['rougeL_to_target'] > 0.3).astype('int')\n",
    "\n",
    "    return generation_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_likelihood_dataframe():\n",
    "    \"\"\"\n",
    "    Load the likelihood data from a pickle file and transform it into a DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A pandas DataFrame containing likelihood data.\n",
    "        sequence_embeddings: Embeddings for each sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct the file path based on run and model names\n",
    "    file_path = f'./log/{run_name}/aggregated_likelihoods_{model_name}_generations.pkl'\n",
    "    \n",
    "    # Load the pickle file into a dictionary\n",
    "    with open(file_path, 'rb') as file:\n",
    "        likelihood_data = pickle.load(file)\n",
    "        print(likelihood_data.keys())\n",
    "    \n",
    "    # Generate subset keys for various metrics\n",
    "    metrics = ['average_predictive_entropy', 'predictive_entropy', 'semantic_predictive_entropy', 'number_of_semantic_sets']\n",
    "    subset_keys = [f\"{metric}_on_subset_{i}\" for metric in metrics for i in range(1, num_generations + 1)]\n",
    "    \n",
    "    # Define the primary keys to use\n",
    "    primary_keys = ('ids', 'predictive_entropy', 'mutual_information', 'average_predictive_entropy',\n",
    "                    'average_pointwise_mutual_information', 'average_neg_log_likelihood_of_most_likely_gen',\n",
    "                    'average_neg_log_likelihood_of_second_most_likely_gen', 'neg_log_likelihood_of_most_likely_gen',\n",
    "                    'predictive_entropy_over_concepts', 'number_of_semantic_sets', 'unnormalised_entropy_over_concepts')\n",
    "    \n",
    "    # Extract the relevant data from the likelihood data\n",
    "    filtered_likelihood_data = {k: likelihood_data[k] for k in primary_keys + tuple(subset_keys)}\n",
    "    \n",
    "    # Convert torch tensors to CPU tensors and squeeze them\n",
    "    for key, value in filtered_likelihood_data.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            filtered_likelihood_data[key] = torch.squeeze(value.cpu())\n",
    "    \n",
    "    # Extract sequence embeddings\n",
    "    sequence_embeddings = likelihood_data['sequence_embeddings']\n",
    "    \n",
    "    # Convert the filtered likelihood data to a DataFrame\n",
    "    likelihood_dataframe = pd.DataFrame.from_dict(filtered_likelihood_data)\n",
    "    \n",
    "    # Rename the 'ids' column to 'id'\n",
    "    likelihood_dataframe.rename(columns={'ids': 'id'}, inplace=True)\n",
    "\n",
    "    return likelihood_dataframe, sequence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['neg_log_likelihoods', 'average_neg_log_likelihoods', 'sequence_embeddings', 'pointwise_mutual_information', 'average_neg_log_likelihood_of_most_likely_gen', 'average_neg_log_likelihood_of_second_most_likely_gen', 'neg_log_likelihood_of_most_likely_gen', 'semantic_set_ids', 'ids', 'mutual_information', 'predictive_entropy', 'predictive_entropy_over_concepts', 'unnormalised_entropy_over_concepts', 'number_of_semantic_sets', 'margin_measures', 'unnormalised_margin_measures', 'average_predictive_entropy', 'average_predictive_entropy_on_subset_1', 'predictive_entropy_on_subset_1', 'semantic_predictive_entropy_on_subset_1', 'number_of_semantic_sets_on_subset_1', 'average_predictive_entropy_on_subset_2', 'predictive_entropy_on_subset_2', 'semantic_predictive_entropy_on_subset_2', 'number_of_semantic_sets_on_subset_2', 'average_predictive_entropy_on_subset_3', 'predictive_entropy_on_subset_3', 'semantic_predictive_entropy_on_subset_3', 'number_of_semantic_sets_on_subset_3', 'average_predictive_entropy_on_subset_4', 'predictive_entropy_on_subset_4', 'semantic_predictive_entropy_on_subset_4', 'number_of_semantic_sets_on_subset_4', 'average_predictive_entropy_on_subset_5', 'predictive_entropy_on_subset_5', 'semantic_predictive_entropy_on_subset_5', 'number_of_semantic_sets_on_subset_5', 'average_pointwise_mutual_information'])\n"
     ]
    }
   ],
   "source": [
    "# Load data from the respective functions\n",
    "similarity_dataframe = load_similarity_dataframe()\n",
    "generation_dataframe = load_generations_dataframe()\n",
    "\n",
    "# Determine the number of generations based on the 'generated_texts' column\n",
    "num_generations = len(generation_dataframe['generated_texts'][0])\n",
    "\n",
    "# Load likelihood data and sequence embeddings\n",
    "likelihood_dataframe, sequence_embeddings = load_likelihood_dataframe()\n",
    "\n",
    "# Merge the dataframes based on the 'id' column to create a comprehensive result dataframe\n",
    "comprehensive_dataframe = generation_dataframe.merge(similarity_dataframe, on='id').merge(likelihood_dataframe, on='id')\n",
    "\n",
    "# Record the number of samples before any filtering\n",
    "n_samples_before_filtering = len(comprehensive_dataframe)\n",
    "\n",
    "# Calculate the length of the most likely generation for each row\n",
    "comprehensive_dataframe['len_most_likely_generation_length'] = comprehensive_dataframe['most_likely_generation'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store analysis results\n",
    "analysis_results = {}\n",
    "analysis_results['accuracy'] = comprehensive_dataframe['correct'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the AUROC (Area Under the Receiver Operating Characteristic) for various metrics\n",
    "\n",
    "# 1. Length Normalized Predictive Entropy\n",
    "ln_predictive_entropy_auroc = sklearn.metrics.roc_auc_score(1 - comprehensive_dataframe['correct'], comprehensive_dataframe['average_predictive_entropy'])\n",
    "analysis_results['ln_predictive_entropy_auroc'] = ln_predictive_entropy_auroc\n",
    "\n",
    "# 2. Predictive Entropy\n",
    "predictive_entropy_auroc = sklearn.metrics.roc_auc_score(1 - comprehensive_dataframe['correct'], comprehensive_dataframe['predictive_entropy'])\n",
    "analysis_results['predictive_entropy_auroc'] = predictive_entropy_auroc\n",
    "\n",
    "# 3. Entropy Over Concepts\n",
    "entropy_over_concepts_auroc = sklearn.metrics.roc_auc_score(1 - comprehensive_dataframe['correct'], comprehensive_dataframe['predictive_entropy_over_concepts'])\n",
    "\n",
    "analysis_results['entropy_over_concepts_auroc'] = entropy_over_concepts_auroc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Unnormalized Entropy Over Concepts (if present in the dataframe)\n",
    "if 'unnormalised_entropy_over_concepts' in comprehensive_dataframe.columns:\n",
    "    unnormalised_entropy_over_concepts_auroc = sklearn.metrics.roc_auc_score(\n",
    "        1 - comprehensive_dataframe['correct'], comprehensive_dataframe['unnormalised_entropy_over_concepts'])\n",
    "    analysis_results['unnormalised_entropy_over_concepts_auroc'] = unnormalised_entropy_over_concepts_auroc\n",
    "\n",
    "# Add the entropy over concepts AUROC to the list for across models comparison\n",
    "aurocs_across_models.append(entropy_over_concepts_auroc)\n",
    "\n",
    "# 5. Negative Log Likelihood of Most Likely Generation\n",
    "neg_llh_most_likely_gen_auroc = sklearn.metrics.roc_auc_score(1 - comprehensive_dataframe['correct'],\n",
    "                                                              comprehensive_dataframe['neg_log_likelihood_of_most_likely_gen'])\n",
    "analysis_results['neg_llh_most_likely_gen_auroc'] = neg_llh_most_likely_gen_auroc\n",
    "\n",
    "# 6. Number of Semantic Sets\n",
    "number_of_semantic_sets_auroc = sklearn.metrics.roc_auc_score(1 - comprehensive_dataframe['correct'],\n",
    "                                                              comprehensive_dataframe['number_of_semantic_sets'])\n",
    "analysis_results['number_of_semantic_sets_auroc'] = number_of_semantic_sets_auroc\n",
    "\n",
    "# Compute average number of semantic sets for correct and incorrect predictions\n",
    "analysis_results['number_of_semantic_sets_correct'] = comprehensive_dataframe[comprehensive_dataframe['correct'] == 1]['number_of_semantic_sets'].mean()\n",
    "analysis_results['number_of_semantic_sets_incorrect'] = comprehensive_dataframe[comprehensive_dataframe['correct'] == 0]['number_of_semantic_sets'].mean()\n",
    "\n",
    "# Compute average Rouge-L scores for all, correct, and incorrect predictions\n",
    "analysis_results['average_rougeL_among_generations'] = comprehensive_dataframe['rougeL_among_generations'].mean()\n",
    "analysis_results['average_rougeL_among_generations_correct'] = comprehensive_dataframe[comprehensive_dataframe['correct'] == 1]['rougeL_among_generations'].mean()\n",
    "analysis_results['average_rougeL_among_generations_incorrect'] = comprehensive_dataframe[comprehensive_dataframe['correct'] == 0]['rougeL_among_generations'].mean()\n",
    "\n",
    "# 7. Rouge-L AUROC\n",
    "analysis_results['average_rougeL_auroc'] = sklearn.metrics.roc_auc_score(comprehensive_dataframe['correct'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Average Negative Log Likelihood of Most Likely Generation\n",
    "average_neg_llh_most_likely_gen_auroc = sklearn.metrics.roc_auc_score(1 - comprehensive_dataframe['correct'], comprehensive_dataframe['average_neg_log_likelihood_of_most_likely_gen'])\n",
    "analysis_results['average_neg_llh_most_likely_gen_auroc'] = average_neg_llh_most_likely_gen_auroc\n",
    "\n",
    "# 9. Rouge-L based accuracy\n",
    "analysis_results['rougeL_based_accuracy'] = comprehensive_dataframe['correct'].mean()\n",
    "\n",
    "# 10. Margin Measure AUROC\n",
    "analysis_results['margin_measure_auroc'] = sklearn.metrics.roc_auc_score(1 - comprehensive_dataframe['correct'], comprehensive_dataframe['average_neg_log_likelihood_of_most_likely_gen'] + \n",
    "                                                                         comprehensive_dataframe['average_neg_log_likelihood_of_second_most_likely_gen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 7184\n",
      "3.4509668\n",
      "0.9861466\n",
      "4.7006407\n",
      "ln_predictive_entropy_auroc 0.5700715655231574\n",
      "semantci entropy auroc 0.57555895225158\n",
      "Semantic entropy + 0.5857361771617284\n",
      "RougeL among generations auroc 0.49982581267572435\n",
      "margin measure auroc: 0.5998732476167101\n"
     ]
    }
   ],
   "source": [
    "if args.verbose:\n",
    "    print('Number of samples:', len(result_df))\n",
    "    print(comprehensive_dataframe['predictive_entropy'].mean())\n",
    "    print(comprehensive_dataframe['average_predictive_entropy'].mean())\n",
    "    print(comprehensive_dataframe['predictive_entropy_over_concepts'].mean())\n",
    "    print('ln_predictive_entropy_auroc', ln_predictive_entropy_auroc)\n",
    "    print('semantci entropy auroc', entropy_over_concepts_auroc)\n",
    "    \n",
    "    combined_entropy_auroc = sklearn.metrics.roc_auc_score( 1 - comprehensive_dataframe['correct'], comprehensive_dataframe['predictive_entropy_over_concepts'] - 3 * comprehensive_dataframe['rougeL_among_generations'])\n",
    "    print('Semantic entropy +', combined_entropy_auroc)\n",
    "    \n",
    "    rougeL_auroc = sklearn.metrics.roc_auc_score(comprehensive_dataframe['correct'], comprehensive_dataframe['rougeL_among_generations'])\n",
    "    print('RougeL among generations auroc', rougeL_auroc)\n",
    "    print('margin measure auroc:', comprehensive_dataframe['margin_measure_auroc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store AUROCs and other metrics for different numbers of generations\n",
    "ln_aurocs = []\n",
    "predictive_aurocs = []\n",
    "semantic_entropy_aurocs = []\n",
    "avg_semantic_sets = []\n",
    "avg_semantic_sets_correct = []\n",
    "avg_semantic_sets_incorrect = []\n",
    "\n",
    "# Compute metrics for each subset of generations\n",
    "for i in range(1, num_generations + 1):\n",
    "    subset_suffix = f\"_on_subset_{i}\"\n",
    "    \n",
    "    # Length Normalized Predictive Entropy AUROC\n",
    "    ln_auroc = sklearn.metrics.roc_auc_score(1 - comprehensive_dataframe['correct'], comprehensive_dataframe[f'average_predictive_entropy{subset_suffix}'])\n",
    "    ln_aurocs.append(ln_auroc)\n",
    "    \n",
    "    # Predictive Entropy AUROC\n",
    "    predictive_auroc = sklearn.metrics.roc_auc_score(1 - comprehensive_dataframe['correct'], comprehensive_dataframe[f'predictive_entropy{subset_suffix}'])\n",
    "    predictive_aurocs.append(predictive_auroc)\n",
    "    \n",
    "    # Semantic Predictive Entropy AUROC\n",
    "    semantic_auroc = sklearn.metrics.roc_auc_score(1 - comprehensive_dataframe['correct'], comprehensive_dataframe[f'semantic_predictive_entropy{subset_suffix}'])\n",
    "    semantic_entropy_aurocs.append(semantic_auroc)\n",
    "    \n",
    "    # Average number of semantic sets for all, correct, and incorrect predictions\n",
    "    avg_semantic_sets.append(comprehensive_dataframe[f'number_of_semantic_sets{subset_suffix}'].mean())\n",
    "    avg_semantic_sets_correct.append(comprehensive_dataframe[comprehensive_dataframe['correct'] == 1][f'number_of_semantic_sets{subset_suffix}'].mean())\n",
    "    avg_semantic_sets_incorrect.append(comprehensive_dataframe[comprehensive_dataframe['correct'] == 0][f'number_of_semantic_sets{subset_suffix}'].mean())\n",
    "\n",
    "# Update the analysis results dictionary with the computed metrics\n",
    "analysis_results.update({\n",
    "    'ln_predictive_entropy_auroc_on_subsets': ln_aurocs,\n",
    "    'predictive_entropy_auroc_on_subsets': predictive_aurocs,\n",
    "    'semantic_predictive_entropy_auroc_on_subsets': semantic_entropy_aurocs,\n",
    "    'average_number_of_semantic_sets_on_subsets': avg_semantic_sets,\n",
    "    'average_number_of_semantic_sets_on_subsets_correct': avg_semantic_sets_correct,\n",
    "    'average_number_of_semantic_sets_on_subsets_incorrect': avg_semantic_sets_incorrect,\n",
    "    'model_name': model_name,\n",
    "    'run_name': run_version\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66382b1d52e40b0a20b279dee634a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>average_neg_llh_most_likely_gen_auroc</td><td>▁</td></tr><tr><td>average_rougeL_among_generations</td><td>▁</td></tr><tr><td>average_rougeL_among_generations_correct</td><td>▁</td></tr><tr><td>average_rougeL_among_generations_incorrect</td><td>▁</td></tr><tr><td>average_rougeL_auroc</td><td>▁</td></tr><tr><td>entropy_over_concepts_auroc</td><td>▁</td></tr><tr><td>ln_predictive_entropy_auroc</td><td>▁</td></tr><tr><td>margin_measure_auroc</td><td>▁</td></tr><tr><td>neg_llh_most_likely_gen_auroc</td><td>▁</td></tr><tr><td>number_of_semantic_sets_auroc</td><td>▁</td></tr><tr><td>number_of_semantic_sets_correct</td><td>▁</td></tr><tr><td>number_of_semantic_sets_incorrect</td><td>▁</td></tr><tr><td>predictive_entropy_auroc</td><td>▁</td></tr><tr><td>rougeL_based_accuracy</td><td>▁</td></tr><tr><td>unnormalised_entropy_over_concepts_auroc</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.27492</td></tr><tr><td>average_neg_llh_most_likely_gen_auroc</td><td>0.56815</td></tr><tr><td>average_rougeL_among_generations</td><td>0.08512</td></tr><tr><td>average_rougeL_among_generations_correct</td><td>0.09761</td></tr><tr><td>average_rougeL_among_generations_incorrect</td><td>0.08039</td></tr><tr><td>average_rougeL_auroc</td><td>0.49983</td></tr><tr><td>entropy_over_concepts_auroc</td><td>0.57556</td></tr><tr><td>ln_predictive_entropy_auroc</td><td>0.57007</td></tr><tr><td>margin_measure_auroc</td><td>0.59987</td></tr><tr><td>model_name</td><td>opt-125m</td></tr><tr><td>neg_llh_most_likely_gen_auroc</td><td>0.57811</td></tr><tr><td>number_of_semantic_sets_auroc</td><td>0.55835</td></tr><tr><td>number_of_semantic_sets_correct</td><td>1.31797</td></tr><tr><td>number_of_semantic_sets_incorrect</td><td>1.48589</td></tr><tr><td>predictive_entropy_auroc</td><td>0.5801</td></tr><tr><td>rougeL_based_accuracy</td><td>0.27492</td></tr><tr><td>run_name</td><td>run_1</td></tr><tr><td>unnormalised_entropy_over_concepts_auroc</td><td>0.58348</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run_1</strong> at: <a href='https://wandb.ai/nlp53113/nlg_uncertainty/runs/run_1' target=\"_blank\">https://wandb.ai/nlp53113/nlg_uncertainty/runs/run_1</a><br/>Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230813_020444-run_1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.log(result_dict)\n",
    "\n",
    "# Store the analysis results and sequence embeddings for the current run ID\n",
    "overall_result_dict[run_version] = analysis_results\n",
    "sequence_embeddings_dict[run_version] = sequence_embeddings\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Free up GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./overall_results.json', 'w') as f:\n",
    "    json.dump(overall_result_dict, f)\n",
    "\n",
    "with open('./sequence_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(sequence_embeddings_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns from the comprehensive dataframe for accuracy verification\n",
    "accuracy_check_dataframe = comprehensive_dataframe[['most_likely_generation', 'answer', 'correct']]\n",
    "\n",
    "# Save the accuracy verification data to a CSV file\n",
    "accuracy_check_dataframe.to_csv('accuracy_verification.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
