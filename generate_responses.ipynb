{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   This notebook configures and prepares a pre-trained model, loads and preprocesses a specific dataset (either 'coqa' or 'trivia_qa'). \n",
    "-   Then generates responses to prompts using different decoding methods (such as beam search or greedy decoding). \n",
    "-   The generated responses are evaluated using various metrics like Rouge and exact match, and the results are organized and returned for further analysis or use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import datasets\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Parameter Tuning for NLI\n",
    "params = {\n",
    "    # Question related arguments\n",
    "    'question_type': None,\n",
    "    'generations_per_prompt': 5,\n",
    "    \n",
    "    # Data related arguments\n",
    "    'data_fraction': 0.9,\n",
    "    \n",
    "    # Model related arguments\n",
    "    'model_version': 'opt-350m',\n",
    "    'experiment_id': 'run_1',\n",
    "    'temperature': 1.0,\n",
    "    'beam_count': 5,\n",
    "    'decoding': 'beam_search',\n",
    "    'nucleus_top_p': 1.0,\n",
    "    'data_source': 'coqa'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\dhair\\Desktop\\Data\\UChicago\\Morningstar\\NLPLearning\\Project\\wandb\\run-20230813_135638-run_1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/nlp53113/nlg_uncertainty/runs/run_1' target=\"_blank\">run_1</a></strong> to <a href='https://wandb.ai/nlp53113/nlg_uncertainty' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp53113/nlg_uncertainty' target=\"_blank\">https://wandb.ai/nlp53113/nlg_uncertainty</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp53113/nlg_uncertainty/runs/run_1' target=\"_blank\">https://wandb.ai/nlp53113/nlg_uncertainty/runs/run_1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# We are using wandb to track our experiments\n",
    "wandb.init(project='nlg_uncertainty', id=params['experiment_id'], config=params, resume='allow')\n",
    "\n",
    "run_version = wandb.run.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x168951e14d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Please make sure you are using CUDA enabled GPU for this project\n",
    "device = 'cuda'\n",
    "\n",
    "# Setting the seed value ensures that the results are reproducible across different runs\n",
    "seed_val = 10\n",
    "\n",
    "# Ensuring that the seed is set for Python's hashing, random operations, NumPy, and PyTorch\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_val)\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9641"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect garbage to free up CUDA memory\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2481979392, 6441926656)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please use the models below you have some free memory in your GPU\n",
    "# If you don't have enough memory, then it can lead to a crash (will require restart of the kernel/system)\n",
    "torch.cuda.mem_get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Getting the model from params and loading it to the GPU\n",
    "# We are using Facebook's OPT: Open Pre-trained Transformer Language Models (if you want to use a different model, change the model_version in params and repository info accordingly)\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"facebook/{params['model_version']}\", torch_dtype=torch.float16, cache_dir='./cache_dir').cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"facebook/{params['model_version']}\", use_fast=False, cache_dir='./cache_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coqa\n",
      "opt-350m\n"
     ]
    }
   ],
   "source": [
    "print(params['data_source'])\n",
    "print(params['model_version'])\n",
    "\n",
    "if params['data_source'] == 'coqa':\n",
    "    data = datasets.load_from_disk('./coqa_dataset')\n",
    "    # Since we have multiple questions for each context, we need to map each question to its context\n",
    "    question_mapping = dict(zip(data['id'], data['question']))\n",
    "    \n",
    "elif params['data_source'] == 'trivia_qa':\n",
    "    data = datasets.load_from_disk('./trivia_qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached split indices for dataset at c:\\Users\\dhair\\Desktop\\Data\\UChicago\\Morningstar\\NLPLearning\\Project\\coqa_dataset\\cache-4161dcb704e577e5.arrow and c:\\Users\\dhair\\Desktop\\Data\\UChicago\\Morningstar\\NLPLearning\\Project\\coqa_dataset\\cache-c3ecca6c6b7ff233.arrow\n"
     ]
    }
   ],
   "source": [
    "# Data_fraction parameter is used to specify the fraction of the dataset to be used for training\n",
    "# Splitting the dataset into training and testing sets based on the specified data_fraction\n",
    "if params['data_fraction'] < 1.0:\n",
    "    train_dataset = data.train_test_split(test_size=(1 - params['data_fraction']), seed=seed_val)['train']\n",
    "else:\n",
    "    train_dataset = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the story and question, adding a prefix for the answer\n",
    "def tokenize_story_and_question(samples):\n",
    "    return tokenizer(samples['story'] + ' Q: ' + samples['question'] + ' A:', truncation=False, padding=False)\n",
    "\n",
    "\n",
    "\n",
    "# Apply tokenization to the dataset and set the format for PyTorch\n",
    "def prepare_dataset_for_training(input_dataset):\n",
    "    \n",
    "    # Mapping the tokenization function to the dataset\n",
    "    processed_dataset = input_dataset.map(tokenize_story_and_question, batched=False, load_from_cache_file=False)\n",
    "    \n",
    "    # Setting the format to PyTorch tensors to feed to the model\n",
    "    processed_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'], output_all_columns=True)\n",
    "\n",
    "    return processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22af3edade554672abde7bd3b6872350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7184 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The trivia_qa's train dataset doesn't require any preprocessing, so we can just load it in.\n",
    "if params['data_source'] == 'coqa':\n",
    "    processed_questions  = prepare_dataset_for_training(train_dataset)\n",
    "else:\n",
    "    processed_questions = train_dataset\n",
    "\n",
    "# DataLoader to handle batch training\n",
    "data_loader = torch.utils.data.DataLoader(processed_questions, batch_size=1)\n",
    "\n",
    "# Tokens to be used for framing periods, questions and answers\n",
    "period_tokens = tokenizer('. ')['input_ids'][1]\n",
    "framing_tokens  = ['Ques:', ' Question:', '\\n', 'Ans:', ' Answer:', 'Q:']\n",
    "question_framing_ids = [[tokenizer(token)['input_ids'][1]] for token in framing_tokens ]\n",
    "\n",
    "# Evaluation metrics\n",
    "rouge_metric = evaluate.load('rouge')\n",
    "exact_match = evaluate.load(\"exact_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(model, data_loader, number_of_generations):\n",
    "    \"\"\"Genrates responses for a given model and data loader. \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sequence_max_length = 256\n",
    "        responses = []\n",
    "        \n",
    "        for batch in tqdm.tqdm(data_loader):\n",
    "            \n",
    "            # Reshape input ids for trivia_qa\n",
    "            inputs = torch.cat(batch['input_ids']).to(device).reshape(1, -1) if params['data_source'] == 'trivia_qa' else batch['input_ids'].to(device)\n",
    "            \n",
    "            # Getting refernce response for the given input ids based on the decoding strategy\n",
    "            if params['decoding'] == 'greedy':\n",
    "                most_likely_generation = model.generate(inputs, num_beams=1, do_sample=False, max_length=inputs.shape[1] + sequence_max_length,\n",
    "                                                        eos_token_id=period_tokens, bad_words_ids=question_framing_ids)\n",
    "                \n",
    "            elif params['decoding'] == 'beam_search':\n",
    "                most_likely_generation = model.generate(inputs, num_beams=5, num_return_sequences=2, do_sample=False, max_length=inputs.shape[1] + sequence_max_length,\n",
    "                                                        eos_token_id=period_tokens, bad_words_ids=question_framing_ids)\n",
    "\n",
    "            \n",
    "            # Generating multiple responses for the given input ids\n",
    "            input_length = inputs.shape[1] if params['data_source'] == 'trivia_qa' else batch['input_ids'].shape[1]\n",
    "            generations = torch.ones((number_of_generations, input_length + sequence_max_length), dtype=torch.long, device=device)\n",
    "            \n",
    "            for i in range(number_of_generations):\n",
    "\n",
    "                generation = model.generate(inputs, do_sample=True, num_return_sequences=1, num_beams=params['beam_count'], max_length=inputs.shape[1] + sequence_max_length,\n",
    "                                            eos_token_id=period_tokens, temperature=params['temperature'], bad_words_ids=question_framing_ids, top_p=params['nucleus_top_p'])\n",
    "                generations[i, :generation.shape[1]] = generation\n",
    "\n",
    "            generations = torch.reshape(generations, (-1, number_of_generations, generations.shape[-1]))\n",
    "            \n",
    "            # Decoding the generated responses\n",
    "            for i in range(generations.shape[0]):\n",
    "\n",
    "                # Creating response dictionary based on the data source\n",
    "                if params['data_source'] == 'coqa':\n",
    "                    response_dict = {   \n",
    "                                        'question': question_mapping[batch['id'][0]],\n",
    "                                        'generations': generations[i].to('cpu'),\n",
    "                                        'prompt': batch['input_ids'][i].to('cpu'),\n",
    "                                        'id': batch['id']    \n",
    "                                    }\n",
    "                else:\n",
    "                    few_shot_question = tokenizer.decode(inputs[0])\n",
    "                    question = few_shot_question.split('Question: ')[-1].split('Answer: ')[0]\n",
    "                    response_dict = {\n",
    "                                        'few_shot_question': tokenizer.decode(inputs[0]),\n",
    "                                        'question': question,\n",
    "                                        'generations': generations[i],\n",
    "                                        'prompt': inputs[0],\n",
    "                                        'id': batch['question_id']\n",
    "                                    }\n",
    "\n",
    "                generated_texts = []\n",
    "                \n",
    "                for generation in generations[i]:\n",
    "                    generated_texts.append(tokenizer.decode(generation[len(batch['input_ids'][i]):], skip_special_tokens=True))\n",
    "\n",
    "                # Adding the generated responses to the response dictionary\n",
    "                response_dict['generated_texts'] = generated_texts\n",
    "                response_dict['most_likely_generation_ids'] = most_likely_generation[0].to('cpu')\n",
    "                response_dict['most_likely_generation'] = tokenizer.decode(most_likely_generation[0][len(batch['input_ids'][i]):], skip_special_tokens=True)\n",
    "\n",
    "                response_dict['second_most_likely_generation_ids'] = most_likely_generation[1].to('cpu')\n",
    "                response_dict['second_most_likely_generation'] = tokenizer.decode(most_likely_generation[1][len(batch['input_ids'][i]):], skip_special_tokens=True)\n",
    "\n",
    "                response_dict['semantic_variability_reference_answers'] = batch['semantic_variability'] if 'semantic_variability' in batch else None\n",
    "                \n",
    "                # Calculating rouge scores for the generated responses\n",
    "                rouge_types = ['rouge1', 'rouge2', 'rougeL']\n",
    "                for rouge_type in rouge_types:\n",
    "                    if rouge_type in batch:\n",
    "                        response_dict[rouge_type + '_reference_answers'] = batch[rouge_type]\n",
    "\n",
    "                    else:\n",
    "                        response_dict[rouge_type + '_reference_answers'] = None\n",
    "\n",
    "                    response_dict[rouge_type + '_to_target'] = 0.0\n",
    "\n",
    "                # Calculating exact match score for the generated responses\n",
    "                response_dict['answer'] = batch['answer']['text'] if params['data_source'] == 'coqa' else batch['answer']\n",
    "                response_dict['additional_answers'] = [x[0] for x in batch['additional_answers']] if params['data_source'] == 'coqa' else None\n",
    "\n",
    "                response_dict['exact_match'] = 0.0\n",
    "\n",
    "                reference_answers = batch['answer']['text'] + [x[0] for x in batch['additional_answers']] if params['data_source'] == 'coqa' else batch['answer']\n",
    "\n",
    "                # Evaluating the generated responses using exact match and rouge metrics\n",
    "                for answer in reference_answers:\n",
    "                    predictions = [response_dict['most_likely_generation'].lstrip()]\n",
    "                    references = [answer]\n",
    "                    results = exact_match.compute(predictions=predictions,\n",
    "                                                         references=references,\n",
    "                                                         ignore_case=True,\n",
    "                                                         ignore_punctuation=True)\n",
    "                    response_dict['exact_match'] = max(results['exact_match'], response_dict['exact_match'])\n",
    "                    rouge_results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "                    for rouge_type in rouge_types:\n",
    "                        response_dict[rouge_type + '_to_target'] = max(rouge_results[rouge_type],\n",
    "                                                                       response_dict[rouge_type + '_to_target'])\n",
    "\n",
    "                responses.append(response_dict)\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7184/7184 [5:09:47<00:00,  2.59s/it]   \n"
     ]
    }
   ],
   "source": [
    "responses = generate_responses(model, data_loader, params['generations_per_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "pathlib.Path(f'./sequences/' + run_version).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model = params['model_version']\n",
    "with open(f'./sequences/{run_version}/{model}_generations.pkl', 'wb') as outfile:\n",
    "    pickle.dump(responses, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
